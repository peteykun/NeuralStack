{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Embedding size\n",
    "m = 16\n",
    "\n",
    "# Hidden size\n",
    "hidden_size = 128\n",
    "\n",
    "# Stack initialized?\n",
    "init = False\n",
    "\n",
    "# RNN vars\n",
    "max_length  = 100\n",
    "vocab_size  = 2          # Input vocabulary\n",
    "seq_length  = 20\n",
    "batch_size  = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = None\n",
    "V = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pop operation helpers\n",
    "def check(stack, strengths, remaining, read, idx):\n",
    "    # Bottom of stack\n",
    "    return idx >= 0 #tf.logical_and(idx >= 1, remaining != 0)\n",
    "\n",
    "def update(stack, strengths, remaining, read, idx):\n",
    "    # Amount we can use at this step\n",
    "    this_qty = tf.minimum(remaining, strengths[:,:,idx:idx+1])\n",
    "\n",
    "    # Update read value\n",
    "    old_read = read\n",
    "    read = tf.reshape(read + tf.reshape(this_qty * V[:,:,idx:idx+1], tf.shape(read)), tf.shape(read))  # for shape constraints\n",
    "\n",
    "    # Update remaining strength\n",
    "    remaining = tf.reshape(remaining - this_qty, tf.shape(remaining))\n",
    "\n",
    "    # Update strengths\n",
    "    before = strengths[:,:,:idx]\n",
    "    this   = tf.reshape(tf.sub(strengths[:,:,idx:idx+1], this_qty), (-1, 1, 1))\n",
    "    after  = strengths[:,:,idx+1:]\n",
    "\n",
    "    strengths = tf.reshape(tf.concat(2, [before, this, after]), tf.shape(strengths), name=\"strength_cat\")\n",
    "\n",
    "    # Update index\n",
    "    idx = idx - 1\n",
    "\n",
    "    return (stack, strengths, remaining, read, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stack_update(d, u, v):\n",
    "    '''\n",
    "    Performs an update to the neural stack.\n",
    "    \n",
    "    Args:\n",
    "      d: Push probability.\n",
    "      u: Pop probability.\n",
    "      v: Push value.\n",
    "    \n",
    "    Returns:\n",
    "      r: The value read from the stack.\n",
    "    '''\n",
    "    global s, V, m, init\n",
    "    \n",
    "    # Change shapes\n",
    "    d = tf.expand_dims(d, -1)\n",
    "    u = tf.expand_dims(u, -1)\n",
    "    v = tf.expand_dims(v, -1)\n",
    "    \n",
    "    # Infer batch size\n",
    "    batch_size = tf.shape(d)[0]\n",
    "    \n",
    "    if init:\n",
    "        # Infer stack size\n",
    "        stack_size = tf.shape(V)[2]\n",
    "        \n",
    "        # Perform initializations\n",
    "        read0     = tf.zeros((batch_size, m))      # Read value\n",
    "        idx0      = stack_size - 1                  # Index into the stack\n",
    "        \n",
    "        initialization = (V, s, u, read0, idx0)\n",
    "        pop_operation = tf.while_loop(check, update, initialization)\n",
    "        \n",
    "        # Update strengths and perform read\n",
    "        s = pop_operation[1]\n",
    "        r = pop_operation[3]\n",
    "        \n",
    "        # Perform push\n",
    "        V = tf.concat(2, [V, v])\n",
    "        s = tf.concat(2, [s, d])\n",
    "        \n",
    "    else:\n",
    "        r = tf.zeros((batch_size, m), dtype=np.float32)\n",
    "        init = True\n",
    "        \n",
    "        # Initialize stack\n",
    "        V = v\n",
    "        s = d\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = [tf.placeholder(tf.float32, shape=(batch_size, vocab_size), name=\"inp%i\" % t) for t in range(seq_length)]\n",
    "label = tf.placeholder(tf.float32, shape=(batch_size, 21), name=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For getting the push probability\n",
    "W_d = tf.Variable(tf.random_normal((hidden_size, 1), stddev=0.01))\n",
    "b_d = tf.Variable(np.zeros((1,)), dtype=tf.float32)\n",
    "\n",
    "# For getting the pop probability\n",
    "W_u = tf.Variable(tf.random_normal((hidden_size, 1), stddev=0.01))\n",
    "b_u = tf.Variable(np.zeros((1,)), dtype=tf.float32)\n",
    "\n",
    "# For getting the value to be pushed\n",
    "W_v = tf.Variable(tf.random_normal((hidden_size, m), stddev=0.01))\n",
    "b_v = tf.Variable(np.zeros((m,)), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize first read to all 0's\n",
    "r_t = tf.zeros((batch_size, m))\n",
    "\n",
    "with tf.variable_scope('rnn_unfolding') as varscope:\n",
    "    for input_ in input:\n",
    "        combined_input = tf.concat(1, [input_, r_t])\n",
    "        output, state  = cell(combined_input, state)\n",
    "        \n",
    "        # Calculate d, u, v\n",
    "        d_t = tf.sigmoid(tf.matmul(output, W_d) + b_d)\n",
    "        u_t = tf.sigmoid(tf.matmul(output, W_u) + b_u)\n",
    "        v_t = tf.tanh(tf.matmul(output, W_v) + b_v)\n",
    "        \n",
    "        # Perform stack operation\n",
    "        r_t = stack_update(d_t, u_t, v_t)\n",
    "        \n",
    "        outputs.append(output)\n",
    "        varscope.reuse_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Attach MLP to last output\n",
    "W = tf.Variable(tf.random_normal((128, 21), stddev=0.01))\n",
    "b = tf.Variable(np.zeros((21,)), dtype=tf.float32)\n",
    "\n",
    "output = tf.sigmoid(tf.matmul(outputs[-1], W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss\n",
    "clipped_output         = tf.clip_by_value(output, 1e-10, 1.0)\n",
    "clipped_1_minus_output = tf.clip_by_value(1 - output, 1e-10, 1.0)\n",
    "\n",
    "loss = -tf.reduce_sum(label * tf.log(clipped_output) + (1 - label) * tf.log(clipped_1_minus_output))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(output, 1), tf.argmax(label, 1)), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hotify(vector, output_vocabulary_size):\n",
    "    # Create the result vector\n",
    "    vector_one_hot = np.zeros(list(vector.shape) + [output_vocabulary_size])\n",
    "    \n",
    "    # Use fancy indexing to activate positions\n",
    "    vector_one_hot[list(np.indices(vector.shape)) + [vector]] = 1\n",
    "    \n",
    "    return vector_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:   100 Loss: 149.7176 Accuracy: 12.13%\n",
      "Iteration:   200 Loss: 125.7907 Accuracy: 14.75%\n",
      "Iteration:   300 Loss: 117.5809 Accuracy: 15.44%\n",
      "Iteration:   400 Loss: 113.7701 Accuracy: 15.79%\n",
      "Iteration:   500 Loss: 111.3110 Accuracy: 15.66%\n",
      "Iteration:   600 Loss: 109.5223 Accuracy: 15.97%\n",
      "Iteration:   700 Loss: 107.8388 Accuracy: 16.38%\n",
      "Iteration:   800 Loss: 106.4812 Accuracy: 16.80%\n",
      "Iteration:   900 Loss: 104.9568 Accuracy: 17.70%\n",
      "Iteration:  1000 Loss: 102.9972 Accuracy: 19.36%\n",
      "Iteration:  1100 Loss: 100.8224 Accuracy: 21.54%\n",
      "Iteration:  1200 Loss: 98.6025 Accuracy: 23.78%\n",
      "Iteration:  1300 Loss: 96.3874 Accuracy: 25.79%\n",
      "Iteration:  1400 Loss: 94.1802 Accuracy: 28.13%\n",
      "Iteration:  1500 Loss: 91.9407 Accuracy: 30.72%\n",
      "Iteration:  1600 Loss: 89.7201 Accuracy: 33.27%\n",
      "Iteration:  1700 Loss: 87.6342 Accuracy: 35.36%\n",
      "Iteration:  1800 Loss: 85.4570 Accuracy: 37.82%\n",
      "Iteration:  1900 Loss: 83.3237 Accuracy: 40.09%\n",
      "Iteration:  2000 Loss: 81.2965 Accuracy: 42.11%\n",
      "Iteration:  2100 Loss: 79.2101 Accuracy: 44.21%\n",
      "Iteration:  2200 Loss: 77.1787 Accuracy: 46.17%\n",
      "Iteration:  2300 Loss: 75.2182 Accuracy: 47.97%\n",
      "Iteration:  2400 Loss: 73.3076 Accuracy: 49.68%\n",
      "Iteration:  2500 Loss: 71.5106 Accuracy: 51.24%\n",
      "Iteration:  2600 Loss: 69.7057 Accuracy: 52.79%\n",
      "Iteration:  2700 Loss: 68.0729 Accuracy: 54.15%\n",
      "Iteration:  2800 Loss: 66.4151 Accuracy: 55.56%\n",
      "Iteration:  2900 Loss: 64.8025 Accuracy: 56.88%\n",
      "Iteration:  3000 Loss: 63.2195 Accuracy: 58.14%\n",
      "Iteration:  3100 Loss: 61.8425 Accuracy: 59.19%\n",
      "Iteration:  3200 Loss: 60.4995 Accuracy: 60.23%\n",
      "Iteration:  3300 Loss: 59.1440 Accuracy: 61.30%\n",
      "Iteration:  3400 Loss: 57.7933 Accuracy: 62.33%\n",
      "Iteration:  3500 Loss: 56.4916 Accuracy: 63.30%\n",
      "Iteration:  3600 Loss: 55.2472 Accuracy: 64.22%\n",
      "Iteration:  3700 Loss: 54.0433 Accuracy: 65.10%\n",
      "Iteration:  3800 Loss: 52.8970 Accuracy: 65.92%\n",
      "Iteration:  3900 Loss: 51.7903 Accuracy: 66.71%\n",
      "Iteration:  4000 Loss: 50.8825 Accuracy: 67.35%\n",
      "Iteration:  4100 Loss: 49.8563 Accuracy: 68.07%\n",
      "Iteration:  4200 Loss: 48.8666 Accuracy: 68.77%\n",
      "Iteration:  4300 Loss: 48.1616 Accuracy: 69.25%\n",
      "Iteration:  4400 Loss: 47.2596 Accuracy: 69.89%\n",
      "Iteration:  4500 Loss: 46.3918 Accuracy: 70.50%\n",
      "Iteration:  4600 Loss: 45.5474 Accuracy: 71.09%\n",
      "Iteration:  4700 Loss: 44.7220 Accuracy: 71.65%\n",
      "Iteration:  4800 Loss: 43.9310 Accuracy: 72.19%\n",
      "Iteration:  4900 Loss: 43.1705 Accuracy: 72.70%\n",
      "Iteration:  5000 Loss: 42.4342 Accuracy: 73.20%\n",
      "Iteration:  5100 Loss: 41.7210 Accuracy: 73.68%\n",
      "Iteration:  5200 Loss: 41.0442 Accuracy: 74.13%\n",
      "Iteration:  5300 Loss: 40.6018 Accuracy: 74.43%\n",
      "Iteration:  5400 Loss: 39.9626 Accuracy: 74.86%\n",
      "Iteration:  5500 Loss: 39.3361 Accuracy: 75.28%\n",
      "Iteration:  5600 Loss: 38.7365 Accuracy: 75.68%\n",
      "Iteration:  5700 Loss: 38.1419 Accuracy: 76.08%\n",
      "Iteration:  5800 Loss: 37.5603 Accuracy: 76.46%\n",
      "Iteration:  5900 Loss: 37.0029 Accuracy: 76.83%\n",
      "Iteration:  6000 Loss: 36.4524 Accuracy: 77.19%\n",
      "Iteration:  6100 Loss: 35.9204 Accuracy: 77.54%\n",
      "Iteration:  6200 Loss: 35.4102 Accuracy: 77.87%\n",
      "Iteration:  6300 Loss: 34.9093 Accuracy: 78.19%\n",
      "Iteration:  6400 Loss: 34.5811 Accuracy: 78.41%\n",
      "Iteration:  6500 Loss: 34.1079 Accuracy: 78.72%\n",
      "Iteration:  6600 Loss: 33.6374 Accuracy: 79.03%\n",
      "Iteration:  6700 Loss: 33.1885 Accuracy: 79.32%\n",
      "Iteration:  6800 Loss: 32.7453 Accuracy: 79.61%\n",
      "Iteration:  6900 Loss: 32.3138 Accuracy: 79.89%\n",
      "Iteration:  7000 Loss: 31.8909 Accuracy: 80.16%\n",
      "Iteration:  7100 Loss: 31.4829 Accuracy: 80.43%\n",
      "Iteration:  7200 Loss: 31.0834 Accuracy: 80.68%\n",
      "Iteration:  7300 Loss: 30.6927 Accuracy: 80.94%\n",
      "Iteration:  7400 Loss: 30.3164 Accuracy: 81.18%\n",
      "Iteration:  7500 Loss: 29.9599 Accuracy: 81.40%\n",
      "Iteration:  7600 Loss: 29.6015 Accuracy: 81.63%\n",
      "Iteration:  7700 Loss: 29.2426 Accuracy: 81.86%\n",
      "Iteration:  7800 Loss: 28.8961 Accuracy: 82.09%\n",
      "Iteration:  7900 Loss: 28.5573 Accuracy: 82.30%\n",
      "Iteration:  8000 Loss: 28.4073 Accuracy: 82.41%\n",
      "Iteration:  8100 Loss: 28.0835 Accuracy: 82.62%\n",
      "Iteration:  8200 Loss: 27.7644 Accuracy: 82.82%\n",
      "Iteration:  8300 Loss: 27.4519 Accuracy: 83.02%\n",
      "Iteration:  8400 Loss: 27.1501 Accuracy: 83.21%\n",
      "Iteration:  8500 Loss: 26.8533 Accuracy: 83.40%\n",
      "Iteration:  8600 Loss: 26.5643 Accuracy: 83.58%\n",
      "Iteration:  8700 Loss: 26.2797 Accuracy: 83.76%\n",
      "Iteration:  8800 Loss: 26.0013 Accuracy: 83.94%\n",
      "Iteration:  8900 Loss: 25.7287 Accuracy: 84.11%\n",
      "Iteration:  9000 Loss: 25.4607 Accuracy: 84.28%\n",
      "Iteration:  9100 Loss: 25.2002 Accuracy: 84.44%\n",
      "Iteration:  9200 Loss: 24.9416 Accuracy: 84.61%\n",
      "Iteration:  9300 Loss: 24.6885 Accuracy: 84.77%\n",
      "Iteration:  9400 Loss: 24.6620 Accuracy: 84.82%\n",
      "Iteration:  9500 Loss: 24.4578 Accuracy: 84.95%\n",
      "Iteration:  9600 Loss: 24.2235 Accuracy: 85.10%\n",
      "Iteration:  9700 Loss: 23.9893 Accuracy: 85.25%\n",
      "Iteration:  9800 Loss: 23.7572 Accuracy: 85.40%\n",
      "Iteration:  9900 Loss: 23.5681 Accuracy: 85.52%\n",
      "Iteration: 10000 Loss: 23.3489 Accuracy: 85.66%\n",
      "Iteration: 10100 Loss: 23.1306 Accuracy: 85.80%\n",
      "Iteration: 10200 Loss: 22.9168 Accuracy: 85.93%\n",
      "Iteration: 10300 Loss: 22.7075 Accuracy: 86.07%\n",
      "Iteration: 10400 Loss: 22.5002 Accuracy: 86.20%\n",
      "Iteration: 10500 Loss: 22.2975 Accuracy: 86.32%\n",
      "Iteration: 10600 Loss: 22.0967 Accuracy: 86.45%\n",
      "Iteration: 10700 Loss: 21.9001 Accuracy: 86.57%\n",
      "Iteration: 10800 Loss: 21.7073 Accuracy: 86.70%\n",
      "Iteration: 10900 Loss: 21.5171 Accuracy: 86.82%\n",
      "Iteration: 11000 Loss: 21.3845 Accuracy: 86.90%\n",
      "Iteration: 11100 Loss: 21.2268 Accuracy: 87.00%\n",
      "Iteration: 11200 Loss: 21.0614 Accuracy: 87.10%\n",
      "Iteration: 11300 Loss: 20.8833 Accuracy: 87.22%\n",
      "Iteration: 11400 Loss: 20.7109 Accuracy: 87.32%\n",
      "Iteration: 11500 Loss: 20.5375 Accuracy: 87.43%\n",
      "Iteration: 11600 Loss: 20.3683 Accuracy: 87.54%\n",
      "Iteration: 11700 Loss: 20.2011 Accuracy: 87.64%\n",
      "Iteration: 11800 Loss: 20.0389 Accuracy: 87.75%\n",
      "Iteration: 11900 Loss: 19.8793 Accuracy: 87.85%\n",
      "Iteration: 12000 Loss: 19.7200 Accuracy: 87.95%\n",
      "Iteration: 12100 Loss: 19.6795 Accuracy: 87.98%\n",
      "Iteration: 12200 Loss: 19.5905 Accuracy: 88.04%\n",
      "Iteration: 12300 Loss: 19.4379 Accuracy: 88.14%\n",
      "Iteration: 12400 Loss: 19.2860 Accuracy: 88.23%\n",
      "Iteration: 12500 Loss: 19.1389 Accuracy: 88.33%\n",
      "Iteration: 12600 Loss: 18.9941 Accuracy: 88.42%\n",
      "Iteration: 12700 Loss: 18.8518 Accuracy: 88.51%\n",
      "Iteration: 12800 Loss: 18.7115 Accuracy: 88.59%\n",
      "Iteration: 12900 Loss: 18.5722 Accuracy: 88.68%\n",
      "Iteration: 13000 Loss: 18.4342 Accuracy: 88.76%\n",
      "Iteration: 13100 Loss: 18.2977 Accuracy: 88.85%\n",
      "Iteration: 13200 Loss: 18.1642 Accuracy: 88.93%\n",
      "Iteration: 13300 Loss: 18.0326 Accuracy: 89.01%\n",
      "Iteration: 13400 Loss: 17.9027 Accuracy: 89.09%\n",
      "Iteration: 13500 Loss: 17.7755 Accuracy: 89.17%\n",
      "Iteration: 13600 Loss: 17.6493 Accuracy: 89.25%\n",
      "Iteration: 13700 Loss: 17.8035 Accuracy: 89.18%\n",
      "Iteration: 13800 Loss: 17.6983 Accuracy: 89.26%\n",
      "Iteration: 13900 Loss: 17.5822 Accuracy: 89.33%\n",
      "Iteration: 14000 Loss: 17.4648 Accuracy: 89.41%\n",
      "Iteration: 14100 Loss: 17.3468 Accuracy: 89.48%\n",
      "Iteration: 14200 Loss: 17.2290 Accuracy: 89.55%\n",
      "Iteration: 14300 Loss: 17.1137 Accuracy: 89.63%\n",
      "Iteration: 14400 Loss: 16.9998 Accuracy: 89.70%\n",
      "Iteration: 14500 Loss: 16.8866 Accuracy: 89.77%\n",
      "Iteration: 14600 Loss: 16.7751 Accuracy: 89.84%\n",
      "Iteration: 14700 Loss: 16.8092 Accuracy: 89.82%\n",
      "Iteration: 14800 Loss: 16.7665 Accuracy: 89.86%\n",
      "Iteration: 14900 Loss: 16.6608 Accuracy: 89.92%\n",
      "Iteration: 15000 Loss: 16.5554 Accuracy: 89.99%\n",
      "Iteration: 15100 Loss: 16.4504 Accuracy: 90.05%\n",
      "Iteration: 15200 Loss: 16.3463 Accuracy: 90.12%\n",
      "Iteration: 15300 Loss: 16.2430 Accuracy: 90.18%\n",
      "Iteration: 15400 Loss: 16.1417 Accuracy: 90.24%\n",
      "Iteration: 15500 Loss: 16.0414 Accuracy: 90.31%\n",
      "Iteration: 15600 Loss: 15.9417 Accuracy: 90.37%\n",
      "Iteration: 15700 Loss: 15.8450 Accuracy: 90.43%\n",
      "Iteration: 15800 Loss: 15.7486 Accuracy: 90.49%\n",
      "Iteration: 15900 Loss: 15.6522 Accuracy: 90.55%\n",
      "Iteration: 16000 Loss: 15.5583 Accuracy: 90.60%\n",
      "Iteration: 16100 Loss: 15.4668 Accuracy: 90.66%\n",
      "Iteration: 16200 Loss: 15.3759 Accuracy: 90.72%\n",
      "Iteration: 16300 Loss: 15.2848 Accuracy: 90.77%\n",
      "Iteration: 16400 Loss: 15.1944 Accuracy: 90.83%\n",
      "Iteration: 16500 Loss: 15.1053 Accuracy: 90.88%\n",
      "Iteration: 16600 Loss: 15.0168 Accuracy: 90.94%\n",
      "Iteration: 16700 Loss: 14.9295 Accuracy: 90.99%\n",
      "Iteration: 16800 Loss: 14.8434 Accuracy: 91.04%\n",
      "Iteration: 16900 Loss: 14.7578 Accuracy: 91.09%\n",
      "Iteration: 17000 Loss: 14.6742 Accuracy: 91.15%\n",
      "Iteration: 17100 Loss: 14.7486 Accuracy: 91.11%\n",
      "Iteration: 17200 Loss: 14.6682 Accuracy: 91.16%\n",
      "Iteration: 17300 Loss: 14.5865 Accuracy: 91.21%\n",
      "Iteration: 17400 Loss: 14.5054 Accuracy: 91.26%\n",
      "Iteration: 17500 Loss: 14.4250 Accuracy: 91.31%\n",
      "Iteration: 17600 Loss: 14.3450 Accuracy: 91.36%\n",
      "Iteration: 17700 Loss: 14.2668 Accuracy: 91.41%\n",
      "Iteration: 17800 Loss: 14.1891 Accuracy: 91.45%\n",
      "Iteration: 17900 Loss: 14.1121 Accuracy: 91.50%\n",
      "Iteration: 18000 Loss: 14.0358 Accuracy: 91.55%\n",
      "Iteration: 18100 Loss: 13.9610 Accuracy: 91.59%\n",
      "Iteration: 18200 Loss: 13.8864 Accuracy: 91.64%\n",
      "Iteration: 18300 Loss: 13.8126 Accuracy: 91.69%\n",
      "Iteration: 18400 Loss: 13.7395 Accuracy: 91.73%\n",
      "Iteration: 18500 Loss: 13.6671 Accuracy: 91.77%\n",
      "Iteration: 18600 Loss: 13.5954 Accuracy: 91.82%\n",
      "Iteration: 18700 Loss: 13.5239 Accuracy: 91.86%\n",
      "Iteration: 18800 Loss: 13.4540 Accuracy: 91.90%\n",
      "Iteration: 18900 Loss: 13.3840 Accuracy: 91.95%\n",
      "Iteration: 19000 Loss: 13.3148 Accuracy: 91.99%\n",
      "Iteration: 19100 Loss: 13.2655 Accuracy: 92.02%\n",
      "Iteration: 19200 Loss: 13.2275 Accuracy: 92.05%\n",
      "Iteration: 19300 Loss: 13.1617 Accuracy: 92.09%\n",
      "Iteration: 19400 Loss: 13.0958 Accuracy: 92.13%\n",
      "Iteration: 19500 Loss: 13.0300 Accuracy: 92.17%\n",
      "Iteration: 19600 Loss: 12.9650 Accuracy: 92.21%\n",
      "Iteration: 19700 Loss: 12.9002 Accuracy: 92.25%\n",
      "Iteration: 19800 Loss: 12.8364 Accuracy: 92.28%\n",
      "Iteration: 19900 Loss: 12.7741 Accuracy: 92.32%\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "losses     = []\n",
    "accuracies = []\n",
    "\n",
    "for iter_ in range(2 * 10 ** 4):\n",
    "    # Generate samples randomly\n",
    "    x_   = np.random.randint(0, 2, (batch_size, seq_length))\n",
    "    x_1h = one_hotify(x_, vocab_size)\n",
    "\n",
    "    y_   = np.sum(x_, 1)\n",
    "    y_1h = one_hotify(y_, seq_length + 1)   # because #_output_symbols = seq_length + 1\n",
    "    \n",
    "    # Calculate loss and backprop\n",
    "    feed_dict = {input[t]: x_1h[:,t,:] for t in range(seq_length)}\n",
    "    feed_dict.update({label: y_1h})\n",
    "    loss_t, acc_t, _ = sess.run((loss, accuracy, train_op), feed_dict)\n",
    "    \n",
    "    losses.append(loss_t)\n",
    "    accuracies.append(acc_t * 100)\n",
    "    \n",
    "    if iter_ % (10 ** 2) == 0 and iter_ > 0:\n",
    "        print 'Iteration: %5d Loss: %3.4f Accuracy: %3.2f%%' % (iter_, np.mean(losses), np.mean(accuracies))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
